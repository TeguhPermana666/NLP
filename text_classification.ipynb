{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Basic ML Model for text classification\n",
    "- implement a text classification menggunakan ML algorithm\n",
    "- basic NLP berbasiskan fitur yang bisa di buat dari text dan akan di test dalam sebuah model pada test data yang set ke dalam evaluasi performance\n",
    "\n",
    "untuk mengclasify sbeuah tweet apakah \" hate speech \" atau \" not \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. About the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = pd.read_csv(\"Basic_ML_Model_for_Text_Classification\\\\final_dataset_basicmlmodel.csv\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Things to note**\n",
    "- **label** sebuah column berisikan sebuah target dari variabel atau sebuah value yang di prediksi oleh test data, 1 => **hate speach**, 0 => **not**\n",
    "- **tweet** adalah sebuah text yang berisikan tweet, tweet adalah sebuah main yang akan di NLP   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 .  â #ireland consumer price index (mom) climbed from previous 0.2% to 0.5% in may   #blog #silver #gold #forex\n",
      "2 . we are so selfish. #orlando #standwithorlando #pulseshooting #orlandoshooting #biggerproblems #selfish #heabreaking   #values #love #\n",
      "3 . i get to see my daddy today!!   #80days #gettingfed\n",
      "4 . ouch...junior is angryð#got7 #junior #yugyoem   #omg \n",
      "5 . i am thankful for having a paner. #thankful #positive     \n"
     ]
    }
   ],
   "source": [
    "for index, tweet in enumerate(dataset['tweet'][10:15]):\n",
    "    print(index+1,\".\",tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note :- Noise present in Tweets**\n",
    "\n",
    " - If you look closely, you'll see that there are many hashtags present in the tweets of the form `#` symbol followed by text. We particularly don't need the `#` symbol so we will clean it out.\n",
    " - Also, there are strange symbols like `â` and `ð` in tweet 4. This is actually `unicode` characters that is present in our dataset that we need to get rid of because they don't particularly add anything meaningful.\n",
    " - There are also numerals and percentages .\n",
    "\n",
    "### 2. Data Cleaning\n",
    "\n",
    "Let's clean up the noise in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#used the regex\n",
    "import re\n",
    "# clean text from noise\n",
    "def clean_text(text):\n",
    "    #Filter to allow only aplhabets\n",
    "    text = re.sub(r'[^a-zA-Z\\']',' ',text)\n",
    "    \n",
    "    #Remove Unicode characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]+','',text)\n",
    "    \n",
    "    #convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['clean_text'] = dataset['tweet'].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        @user when a father is dysfunctional and is s...\n",
       "1       @user @user thanks for #lyft credit i can't us...\n",
       "2                                     bihday your majesty\n",
       "3       #model   i love u take with u all the time in ...\n",
       "4                  factsguide: society now    #motivation\n",
       "                              ...                        \n",
       "5237    lady banned from kentucky mall. @user  #jcpenn...\n",
       "5238    @user omfg i'm offended! i'm a  mailbox and i'...\n",
       "5239    @user @user you don't have the balls to hashta...\n",
       "5240     makes you ask yourself, who am i? then am i a...\n",
       "5241    @user #sikh #temple vandalised in in #calgary,...\n",
       "Name: tweet, Length: 5242, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         user when a father is dysfunctional and is s...\n",
       "1        user  user thanks for  lyft credit i can't us...\n",
       "2                                     bihday your majesty\n",
       "3        model   i love u take with u all the time in ...\n",
       "4                  factsguide  society now     motivation\n",
       "                              ...                        \n",
       "5237    lady banned from kentucky mall   user   jcpenn...\n",
       "5238     user omfg i'm offended  i'm a  mailbox and i'...\n",
       "5239     user  user you don't have the balls to hashta...\n",
       "5240     makes you ask yourself  who am i  then am i a...\n",
       "5241     user  sikh  temple vandalised in in  calgary ...\n",
       "Name: clean_text, Length: 5242, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used nlp module\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "def transfor_text(text):\n",
    "    transfor=[]\n",
    "    text = word_tokenize(text)\n",
    "    for i in text:\n",
    "        if i not in stopwords.words(\"english\") and i not in string.punctuation and i.isalnum():\n",
    "            transfor.append(i)\n",
    "    return \" \".join(transfor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       user father dysfunctional selfish drags kids d...\n",
       "1       user user thanks lyft credit ca use cause offe...\n",
       "2                                          bihday majesty\n",
       "3                                model love u take u time\n",
       "4                           factsguide society motivation\n",
       "                              ...                        \n",
       "5237      lady banned kentucky mall user jcpenny kentucky\n",
       "5238    user omfg offended mailbox proud mailboxpride ...\n",
       "5239     user user balls hashtag say weasel lumpy dipshit\n",
       "5240                       makes ask anybody oh thank god\n",
       "5241    user sikh temple vandalised calgary wso condem...\n",
       "Name: tweet, Length: 5242, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transfor_data = dataset[\"tweet\"].apply(transfor_text)\n",
    "transfor_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sama aja\n",
    "\n",
    "cuma kalau regex bakal harus ngerubah ke str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " \"can't\",\n",
       " 'cannot',\n",
       " 'com',\n",
       " 'could',\n",
       " \"couldn't\",\n",
       " 'did',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'else',\n",
       " 'ever',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'get',\n",
       " 'had',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " \"he's\",\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " \"here's\",\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " \"how's\",\n",
       " 'however',\n",
       " 'http',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " \"i've\",\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'k',\n",
       " \"let's\",\n",
       " 'like',\n",
       " 'me',\n",
       " 'more',\n",
       " 'most',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'otherwise',\n",
       " 'ought',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'r',\n",
       " 'same',\n",
       " 'shall',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"shouldn't\",\n",
       " 'since',\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that's\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " \"there's\",\n",
       " 'therefore',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'very',\n",
       " 'was',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " \"we've\",\n",
       " 'were',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " \"what's\",\n",
       " 'when',\n",
       " \"when's\",\n",
       " 'where',\n",
       " \"where's\",\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " \"who's\",\n",
       " 'whom',\n",
       " 'why',\n",
       " \"why's\",\n",
       " 'with',\n",
       " \"won't\",\n",
       " 'would',\n",
       " \"wouldn't\",\n",
       " 'www',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from wordcloud import STOPWORDS\n",
    "STOP_WORDS = STOPWORDS\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cek counts of words at the tweet\n",
    "def gen_freq(text):\n",
    "    #ill store the list of words\n",
    "    word_list = []\n",
    "    \n",
    "    #loop over all the tweets and extract word into word_list\n",
    "    for tw_words in text.split():#split sama kayak word tokenize\n",
    "        word_list.extend(tw_words)\n",
    "    \n",
    "    # create word frequencies using word list\n",
    "    word_freq = pd.Series(word_list).value_counts()\n",
    "    \n",
    "    # drop stop words in english during frequency calculation\n",
    "    word_freq = word_freq.drop(STOP_WORDS,errors='ignore')\n",
    "    \n",
    "    return word_freq\n",
    "\n",
    "# check whether a negation term is present in the text\n",
    "def any_neg(words):\n",
    "    for word in words:\n",
    "        if word in ['n','no', 'non','not'] or re.search(r\"\\wn't\",word):\n",
    "            return 1 # yang mengandung negasi -> auto high spech\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# chech whether one of the 100 rare words is present in the text\n",
    "def any_rare(words,rare_100):\n",
    "    for word in words :\n",
    "        if word in rare_100:\n",
    "            return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "# chech whether prompt words are present\n",
    "def is_qeustion(words):\n",
    "    for word in words :\n",
    "        if word in ['when','what','how','who','why','where']:\n",
    "            return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user      3351\n",
       "amp        439\n",
       "love       320\n",
       "day        254\n",
       "trump      214\n",
       "happy      207\n",
       "will       191\n",
       "people     186\n",
       "new        171\n",
       "u          158\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# word freq\n",
    "word_freq = gen_freq(dataset['clean_text'].str)\n",
    "word_freq[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lou          1\n",
       "stlislou     1\n",
       "semperfi     1\n",
       "packaging    1\n",
       "nighty       1\n",
       "            ..\n",
       "wwefi        1\n",
       "mitb         1\n",
       "superkick    1\n",
       "cssday       1\n",
       "dipshit      1\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100 most rare words in dataset\n",
    "rare_100 = word_freq[-100:]\n",
    "rare_100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       18\n",
       "1       21\n",
       "2        3\n",
       "3       12\n",
       "4        4\n",
       "        ..\n",
       "5237     8\n",
       "5238    15\n",
       "5239    24\n",
       "5240    17\n",
       "5241    10\n",
       "Name: word_count, Length: 5242, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of word in a tweet\n",
    "dataset['word_count'] = dataset['clean_text'].apply(lambda x : len(word_tokenize(x)))\n",
    "dataset['word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [user, when, a, father, is, dysfunctional, and...\n",
       "1       [user, user, thanks, for, lyft, credit, i, can...\n",
       "2                                 [bihday, your, majesty]\n",
       "3       [model, i, love, u, take, with, u, all, the, t...\n",
       "4                  [factsguide, society, now, motivation]\n",
       "                              ...                        \n",
       "5237    [lady, banned, from, kentucky, mall, user, jcp...\n",
       "5238    [user, omfg, i'm, offended, i'm, a, mailbox, a...\n",
       "5239    [user, user, you, don't, have, the, balls, to,...\n",
       "5240    [makes, you, ask, yourself, who, am, i, then, ...\n",
       "5241    [user, sikh, temple, vandalised, in, in, calga...\n",
       "Name: clean_text, Length: 5242, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['clean_text'].str.split() # mirip kayak pakai word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       1\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5237    0\n",
       "5238    0\n",
       "5239    1\n",
       "5240    0\n",
       "5241    0\n",
       "Name: any_neg, Length: 5242, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#negation present or not\n",
    "dataset['any_neg'] = dataset['clean_text'].str.split().apply(lambda x : any_neg(x))\n",
    "dataset['any_neg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1\n",
       "1       0\n",
       "2       0\n",
       "3       0\n",
       "4       0\n",
       "       ..\n",
       "5237    0\n",
       "5238    0\n",
       "5239    0\n",
       "5240    1\n",
       "5241    0\n",
       "Name: is_qeustion, Length: 5242, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# present prompt or not\n",
    "dataset['is_qeustion'] = dataset['clean_text'].str.split().apply(lambda x : is_qeustion(x))\n",
    "dataset['is_qeustion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present rare -> rare adalah sebuah value counts yang tidak duplicate\n",
    "dataset['any_rare'] = dataset['clean_text'].str.split().apply(lambda x : any_rare(x,rare_100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Character count of the tweet\n",
    "dataset['char_count'] = dataset['clean_text'].apply(lambda x : len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "      <th>any_neg</th>\n",
       "      <th>is_qeustion</th>\n",
       "      <th>any_rare</th>\n",
       "      <th>char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "      <td>user when a father is dysfunctional and is s...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "      <td>user  user thanks for  lyft credit i can't us...</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "      <td>model   i love u take with u all the time in ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide  society now     motivation</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5237</th>\n",
       "      <td>31935</td>\n",
       "      <td>1</td>\n",
       "      <td>lady banned from kentucky mall. @user  #jcpenn...</td>\n",
       "      <td>lady banned from kentucky mall   user   jcpenn...</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5238</th>\n",
       "      <td>31947</td>\n",
       "      <td>1</td>\n",
       "      <td>@user omfg i'm offended! i'm a  mailbox and i'...</td>\n",
       "      <td>user omfg i'm offended  i'm a  mailbox and i'...</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5239</th>\n",
       "      <td>31948</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user you don't have the balls to hashta...</td>\n",
       "      <td>user  user you don't have the balls to hashta...</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5240</th>\n",
       "      <td>31949</td>\n",
       "      <td>1</td>\n",
       "      <td>makes you ask yourself, who am i? then am i a...</td>\n",
       "      <td>makes you ask yourself  who am i  then am i a...</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5241</th>\n",
       "      <td>31961</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #sikh #temple vandalised in in #calgary,...</td>\n",
       "      <td>user  sikh  temple vandalised in in  calgary ...</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5242 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  label                                              tweet  \\\n",
       "0         1      0   @user when a father is dysfunctional and is s...   \n",
       "1         2      0  @user @user thanks for #lyft credit i can't us...   \n",
       "2         3      0                                bihday your majesty   \n",
       "3         4      0  #model   i love u take with u all the time in ...   \n",
       "4         5      0             factsguide: society now    #motivation   \n",
       "...     ...    ...                                                ...   \n",
       "5237  31935      1  lady banned from kentucky mall. @user  #jcpenn...   \n",
       "5238  31947      1  @user omfg i'm offended! i'm a  mailbox and i'...   \n",
       "5239  31948      1  @user @user you don't have the balls to hashta...   \n",
       "5240  31949      1   makes you ask yourself, who am i? then am i a...   \n",
       "5241  31961      1  @user #sikh #temple vandalised in in #calgary,...   \n",
       "\n",
       "                                             clean_text  word_count  any_neg  \\\n",
       "0       user when a father is dysfunctional and is s...          18        0   \n",
       "1      user  user thanks for  lyft credit i can't us...          21        1   \n",
       "2                                   bihday your majesty           3        0   \n",
       "3      model   i love u take with u all the time in ...          12        0   \n",
       "4                factsguide  society now     motivation           4        0   \n",
       "...                                                 ...         ...      ...   \n",
       "5237  lady banned from kentucky mall   user   jcpenn...           8        0   \n",
       "5238   user omfg i'm offended  i'm a  mailbox and i'...          15        0   \n",
       "5239   user  user you don't have the balls to hashta...          24        1   \n",
       "5240   makes you ask yourself  who am i  then am i a...          17        0   \n",
       "5241   user  sikh  temple vandalised in in  calgary ...          10        0   \n",
       "\n",
       "      is_qeustion  any_rare  char_count  \n",
       "0               1         0         102  \n",
       "1               0         0         122  \n",
       "2               0         0          21  \n",
       "3               0         0          86  \n",
       "4               0         0          39  \n",
       "...           ...       ...         ...  \n",
       "5237            0         0          59  \n",
       "5238            0         0          82  \n",
       "5239            0         1         112  \n",
       "5240            1         0          87  \n",
       "5241            0         0          67  \n",
       "\n",
       "[5242 rows x 9 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# numerical data\n",
    "X = dataset[['word_count','any_neg','is_qeustion','any_rare','char_count']]\n",
    "y = dataset['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Train a ML model for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "# initialize gauusian NB classifier\n",
    "model = GaussianNB()\n",
    "# fit the model on the train data set\n",
    "model = model.fit(X_train,y_train)\n",
    "# make prediciton on test data set\n",
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 44.952380952380956 %  | F1 Score :  60.893098782138026 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score,accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, pred)*100, \"%\", \" | F1 Score : \", f1_score(y_test,pred)*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model,open(\"model.pkl\",\"wb\"))\n",
    "pickle.dump(dataset,open(\"dataset.pkl\",\"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1d9910303eecfe3b6cdcbdaa4d482c9110c4a3a39e4f394d9897f708ec021033"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
